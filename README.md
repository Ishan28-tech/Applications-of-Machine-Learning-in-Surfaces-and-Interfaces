ğŸ“˜ Applications of Machine Learning in Surfaces and Interfaces
ğŸš€ PROJECT OVERVIEW

This project applies Machine Learning (ML) techniques to predict adsorption energy of different adsorbates on metal surfaces.
Understanding adsorption behavior is crucial for:

Catalysis

Surface chemistry

Material design

ğŸ”¥ BASELINE MODELS

Baseline ML models were trained using structural and chemical features to estimate adsorption energy and compare performance.

Models Trained

Linear Regression

Random Forest Regressor

Tuned Random Forest (GridSearchCV)

Gradient Boosting Regressor

ğŸ“Š BASELINE MODEL EVALUATION METRICS
Model	MAE â†“	RMSE â†“	RÂ² â†‘
Linear Regression	~0.48	~0.71	~0.88
Random Forest	~0.44	~0.77	~0.86
Tuned Random Forest	~0.45	~0.78	~0.86
Gradient Boosting	~0.68	~0.91	~0.81

âœ” Linear Regression and Random Forest gave the best balance of accuracy and simplicity.

ğŸ§ª TEST THE SAVED BASELINE MODEL
import joblib
import pandas as pd

model = joblib.load("models/best_model.pkl")

demo = pd.DataFrame([{
    "Element": "Cu",
    "Adsorbate Smiles": "O=O",
    "h": 1, "k": 1, "l": 1,
    "Surface Shift": 0
}])

print("Predicted Energy:", model.predict(demo)[0])

ğŸ”¥ ADVANCED MODELS

After completing the baseline model, a set of advanced ML models was trained for improved accuracy and generalization.

Models Implemented

XGBoost Regressor

LightGBM Regressor

CatBoost Regressor

Neural Network (MLPRegressor)

These models capture more complex nonlinear patterns in the adsorption dataset.

ğŸ“Š ADVANCED MODEL EVALUATION METRICS
Model	MAE â†“	RMSE â†“	RÂ² â†‘	Performance Summary
XGBoost	~0.554	~0.930	~0.809	â­ Best model â€” excellent fit, low error, high RÂ²
CatBoost	~0.618	~0.953	~0.800	Strong performance, slightly higher error than XGBoost
Neural Network (MLP)	~0.611	~0.974	~0.791	Good performance, may improve with tuning
LightGBM	~1.054	~1.560	~0.463	Underperformed â€” sensitive to dataset size
âœ” Best Advanced Model: XGBoost

Why XGBoost wins:

Handles nonlinear interactions extremely well

Built-in regularization prevents overfitting

Optimized tree boosting = better accuracy

Robust even with small datasets

ğŸ“‰ VISUALIZATIONS
Predicted vs Actual (XGBoost)

(Insert your image)

Predicted vs Actual â€“ XGBoost

Advanced Model Comparison (Bar Chart)

(Insert your image)

Advanced Model Comparison

ğŸ“ NEW FILES ADDED (Advanced Model Section)

Your repository now includes:

src/advanced_models.ipynb â†’ Full advanced model pipeline

models/best_xgb.pkl â†’ Best model saved

models/best_cat.pkl â†’ CatBoost model

(Optional) /assets â†’ Plots and visualizations

This keeps the project organized and scalable.

ğŸ— WORKFLOW SUMMARY

The complete ML pipeline includes:

Dataset Loading & Cleaning

Feature Engineering

Encoding & Scaling

Baseline Model Training

Advanced Model Training (XGBoost, CatBoost, NN)

Model Evaluation (MAE, RMSE, RÂ²)

Model Saving

Visualization & Interpretation

âš™ï¸ TESTING THE ADVANCED MODEL
import joblib
import pandas as pd

model = joblib.load("models/best_xgb.pkl")

demo = pd.DataFrame([{
    "Element": "Cu",
    "Adsorbate Smiles": "O=O",
    "h": 1, "k": 1, "l": 1,
    "Surface Shift": 0
}])

print("Predicted Energy:", model.predict(demo)[0])

ğŸ”® FUTURE SCOPE OF THE PROJECT

Expand dataset to multi-element alloy surfaces

Include DFT-derived physical descriptors

Apply Graph Neural Networks (GNNs)

Build a web-based prediction app

Integrate DFT + ML hybrid pipelines

ğŸ§¾ CONCLUSION

This project demonstrates that advanced ML modelsâ€”especially XGBoostâ€”can reliably predict adsorption energies with high accuracy.
Such models significantly reduce computational cost and accelerate research in:

Surface science

Catalysis

Interface engineering

Materials discovery

ğŸ”§ FUTURE WORK

Extend dataset to multi-element alloy surfaces

Integrate GNNs for atomic-level understanding

Develop hybrid DFT + ML methodologies

Deploy a web-based prediction tool
